{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "German-to-English.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ek54lkEzneI",
        "outputId": "6bbc0930-93e2-436c-a958-438c8f1882dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-04-12 02:19:05--  http://www.manythings.org/anki/deu-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.92.44, 172.67.186.54, 2606:4700:3033::ac43:ba36, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.92.44|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9347491 (8.9M) [application/zip]\n",
            "Saving to: ‘deu-eng.zip’\n",
            "\n",
            "deu-eng.zip         100%[===================>]   8.91M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-04-12 02:19:05 (60.9 MB/s) - ‘deu-eng.zip’ saved [9347491/9347491]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://www.manythings.org/anki/deu-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump,load\n",
        "from unicodedata import normalize\n",
        "from numpy import array,argmax\n",
        "from zipfile import ZipFile\n",
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "Wb8az-vb0egr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = \"deu-eng.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "    print('Extracting all the files now...')\n",
        "    zip.extractall()\n",
        "    print('Done!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whB1kCGyz9xO",
        "outputId": "258e5f7e-e3b4-4f79-e659-e5dc12ccb418"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_doc(filename):\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "metadata": {
        "id": "rjqC86BW0CKo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in lines]\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "hs_ibtxj0MWj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_pairs(lines):\n",
        "    cleaned = list()\n",
        "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    for pair in lines:\n",
        "        clean_pair = list()\n",
        "        for line in pair:\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            line = line.split()\n",
        "            line = [word.lower() for word in line]\n",
        "            line = [re_punc.sub('', w) for w in line]\n",
        "            line = [re_print.sub('', w) for w in line]\n",
        "            line = [word for word in line if word.isalpha()]\n",
        "            clean_pair.append(' '.join(line))\n",
        "        cleaned.append(clean_pair)\n",
        "    return array(cleaned)"
      ],
      "metadata": {
        "id": "NEngchd60SIk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved:',filename)\n",
        "\n",
        "filename = 'deu.txt'\n",
        "doc = load_doc(filename)\n",
        "pairs = to_pairs(doc)\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "save_clean_data(clean_pairs, 'english-german.pkl')\n",
        "for i in range(100):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSBnRGLs0iUL",
        "outputId": "d34fda92-a916-4972-a006-46b846c68d1d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german.pkl\n",
            "[go] => [geh]\n",
            "[hi] => [hallo]\n",
            "[hi] => [gru gott]\n",
            "[run] => [lauf]\n",
            "[run] => [lauf]\n",
            "[wow] => [potzdonner]\n",
            "[wow] => [donnerwetter]\n",
            "[duck] => [kopf runter]\n",
            "[fire] => [feuer]\n",
            "[help] => [hilfe]\n",
            "[help] => [zu hulf]\n",
            "[stay] => [bleib]\n",
            "[stop] => [stopp]\n",
            "[stop] => [anhalten]\n",
            "[wait] => [warte]\n",
            "[wait] => [warte]\n",
            "[begin] => [fang an]\n",
            "[do it] => [mache es]\n",
            "[do it] => [tue es]\n",
            "[go on] => [mach weiter]\n",
            "[hello] => [hallo]\n",
            "[hello] => [sers]\n",
            "[hurry] => [beeil dich]\n",
            "[hurry] => [schnell]\n",
            "[i hid] => [ich versteckte mich]\n",
            "[i hid] => [ich habe mich versteckt]\n",
            "[i ran] => [ich rannte]\n",
            "[i see] => [ich verstehe]\n",
            "[i see] => [aha]\n",
            "[i try] => [ich versuche es]\n",
            "[i try] => [ich probiere es]\n",
            "[i won] => [ich hab gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[i won] => [ich habe gewonnen]\n",
            "[oh no] => [oh nein]\n",
            "[relax] => [entspann dich]\n",
            "[shoot] => [feuer]\n",
            "[shoot] => [schie]\n",
            "[smile] => [lacheln]\n",
            "[sorry] => [entschuldigung]\n",
            "[ask me] => [frag mich]\n",
            "[ask me] => [fragt mich]\n",
            "[ask me] => [fragen sie mich]\n",
            "[attack] => [angriff]\n",
            "[attack] => [attacke]\n",
            "[buy it] => [kaufs]\n",
            "[cheers] => [zum wohl]\n",
            "[eat it] => [iss es]\n",
            "[eat up] => [iss fertig]\n",
            "[eat up] => [iss auf]\n",
            "[eat up] => [iss auf]\n",
            "[freeze] => [keine bewegung]\n",
            "[freeze] => [stehenbleiben]\n",
            "[go now] => [geh jetzt]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [ich habs]\n",
            "[got it] => [aha]\n",
            "[got it] => [kapiert]\n",
            "[got it] => [verstanden]\n",
            "[got it] => [einverstanden]\n",
            "[he ran] => [er rannte]\n",
            "[he ran] => [er lief]\n",
            "[hop in] => [mach mit]\n",
            "[hop in] => [spring rein]\n",
            "[hug me] => [druck mich]\n",
            "[hug me] => [nimm mich in den arm]\n",
            "[hug me] => [umarme mich]\n",
            "[i care] => [mir ist es wichtig]\n",
            "[i fell] => [ich fiel]\n",
            "[i fell] => [ich fiel hin]\n",
            "[i fell] => [ich sturzte]\n",
            "[i fell] => [ich bin hingefallen]\n",
            "[i fell] => [ich bin gesturzt]\n",
            "[i fled] => [ich fluchtete]\n",
            "[i fled] => [ich bin gefluchtet]\n",
            "[i know] => [ich wei]\n",
            "[i lied] => [ich habe gelogen]\n",
            "[i lost] => [ich habe verloren]\n",
            "[i paid] => [ich habe bezahlt]\n",
            "[i paid] => [ich zahlte]\n",
            "[i sang] => [ich sang]\n",
            "[i spit] => [ich spuckte]\n",
            "[i spit] => [ich habe gespuckt]\n",
            "[i swim] => [ich schwimme]\n",
            "[i wept] => [ich weinte]\n",
            "[i wept] => [ich habe geweint]\n",
            "[im] => [ich bin jahre alt]\n",
            "[im] => [ich bin]\n",
            "[im ok] => [mir gehts gut]\n",
            "[im ok] => [es geht mir gut]\n",
            "[im up] => [ich bin wach]\n",
            "[im up] => [ich bin auf]\n",
            "[listen] => [hort zu]\n",
            "[no way] => [unmoglich]\n",
            "[no way] => [das kommt nicht in frage]\n",
            "[no way] => [das gibts doch nicht]\n",
            "[no way] => [ausgeschlossen]\n",
            "[no way] => [in keinster weise]\n",
            "[really] => [wirklich]\n",
            "[really] => [echt]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "raw_dataset = load_clean_sentences('english-german.pkl')\n",
        "\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :2]\n",
        "\n",
        "shuffle(dataset)\n",
        "\n",
        "train, test = dataset[:9000], dataset[9000:]\n",
        "\n",
        "save_clean_data(dataset, 'english-german-both.pkl')\n",
        "save_clean_data(train, 'english-german-train.pkl')\n",
        "save_clean_data(test, 'english-german-test.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c84F_hab058K",
        "outputId": "4d576e8c-7c7b-4a96-a53d-f6db34311e61"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: english-german-both.pkl\n",
            "Saved: english-german-train.pkl\n",
            "Saved: english-german-test.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')"
      ],
      "metadata": {
        "id": "PFz2AKhq1JvV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "def encode_output(sequences, vocab_size):\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y\n",
        "\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcSZOA9u1RPo",
        "outputId": "54fdd544-2cd1-42fa-be00-f9db3cb38051"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2171\n",
            "English Max Length: 5\n",
            "German Vocabulary Size: 3533\n",
            "German Max Length: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kS4alAou5qkl",
        "outputId": "d3ae50b2-8694-4eec-b554-a500e2d82138"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['help yourself', 'bedien dich'],\n",
              "       ['come see me', 'komm und besuch mich'],\n",
              "       ['wish me luck', 'druckt mir die daumen'],\n",
              "       ...,\n",
              "       ['im so excited', 'ich bin ja so aufgeregt'],\n",
              "       ['i ate too much', 'ich habe zu viel gegessen'],\n",
              "       ['youve tried', 'du hast es versucht']], dtype='<U527')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(n_units))\n",
        "    model.add(RepeatVector(tar_timesteps))\n",
        "    model.add(LSTM(n_units, return_sequences=True))\n",
        "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "    model.summary()\n",
        "    return model"
      ],
      "metadata": {
        "id": "NdYPPNxC1obO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('German Max Length: %d' % (ger_length))\n",
        "\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "\n",
        "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1,save_best_only=True, mode='min')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZx_hGNu2fEN",
        "outputId": "51b72106-f7e0-44c8-9f4a-142f10a94831"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 2171\n",
            "English Max Length: 5\n",
            "German Vocabulary Size: 3533\n",
            "German Max Length: 9\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 9, 256)            904448    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 256)               525312    \n",
            "                                                                 \n",
            " repeat_vector (RepeatVector  (None, 5, 256)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 5, 256)            525312    \n",
            "                                                                 \n",
            " time_distributed (TimeDistr  (None, 5, 2171)          557947    \n",
            " ibuted)                                                         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,513,019\n",
            "Trainable params: 2,513,019\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),callbacks=[checkpoint], verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE7j4ci_29UL",
        "outputId": "34d7bac1-7465-4546-8755-d9ec917c9d1e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\n",
            "Epoch 1: val_loss improved from inf to 3.36337, saving model to model.h5\n",
            "141/141 - 15s - loss: 4.1122 - val_loss: 3.3634 - 15s/epoch - 109ms/step\n",
            "Epoch 2/30\n",
            "\n",
            "Epoch 2: val_loss improved from 3.36337 to 3.21857, saving model to model.h5\n",
            "141/141 - 3s - loss: 3.1992 - val_loss: 3.2186 - 3s/epoch - 19ms/step\n",
            "Epoch 3/30\n",
            "\n",
            "Epoch 3: val_loss improved from 3.21857 to 3.09473, saving model to model.h5\n",
            "141/141 - 3s - loss: 3.0484 - val_loss: 3.0947 - 3s/epoch - 18ms/step\n",
            "Epoch 4/30\n",
            "\n",
            "Epoch 4: val_loss improved from 3.09473 to 2.97421, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.8908 - val_loss: 2.9742 - 3s/epoch - 18ms/step\n",
            "Epoch 5/30\n",
            "\n",
            "Epoch 5: val_loss improved from 2.97421 to 2.87719, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.7561 - val_loss: 2.8772 - 3s/epoch - 18ms/step\n",
            "Epoch 6/30\n",
            "\n",
            "Epoch 6: val_loss improved from 2.87719 to 2.80320, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.6282 - val_loss: 2.8032 - 3s/epoch - 18ms/step\n",
            "Epoch 7/30\n",
            "\n",
            "Epoch 7: val_loss improved from 2.80320 to 2.69798, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.4941 - val_loss: 2.6980 - 3s/epoch - 18ms/step\n",
            "Epoch 8/30\n",
            "\n",
            "Epoch 8: val_loss improved from 2.69798 to 2.58594, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.3552 - val_loss: 2.5859 - 3s/epoch - 18ms/step\n",
            "Epoch 9/30\n",
            "\n",
            "Epoch 9: val_loss improved from 2.58594 to 2.49263, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.1979 - val_loss: 2.4926 - 3s/epoch - 18ms/step\n",
            "Epoch 10/30\n",
            "\n",
            "Epoch 10: val_loss improved from 2.49263 to 2.40609, saving model to model.h5\n",
            "141/141 - 3s - loss: 2.0547 - val_loss: 2.4061 - 3s/epoch - 18ms/step\n",
            "Epoch 11/30\n",
            "\n",
            "Epoch 11: val_loss improved from 2.40609 to 2.30956, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.9169 - val_loss: 2.3096 - 3s/epoch - 18ms/step\n",
            "Epoch 12/30\n",
            "\n",
            "Epoch 12: val_loss improved from 2.30956 to 2.25330, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.7912 - val_loss: 2.2533 - 3s/epoch - 19ms/step\n",
            "Epoch 13/30\n",
            "\n",
            "Epoch 13: val_loss improved from 2.25330 to 2.19263, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.6718 - val_loss: 2.1926 - 3s/epoch - 19ms/step\n",
            "Epoch 14/30\n",
            "\n",
            "Epoch 14: val_loss improved from 2.19263 to 2.18037, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.5636 - val_loss: 2.1804 - 3s/epoch - 19ms/step\n",
            "Epoch 15/30\n",
            "\n",
            "Epoch 15: val_loss improved from 2.18037 to 2.11696, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.4696 - val_loss: 2.1170 - 3s/epoch - 19ms/step\n",
            "Epoch 16/30\n",
            "\n",
            "Epoch 16: val_loss improved from 2.11696 to 2.08014, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.3706 - val_loss: 2.0801 - 3s/epoch - 19ms/step\n",
            "Epoch 17/30\n",
            "\n",
            "Epoch 17: val_loss improved from 2.08014 to 2.03251, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.2746 - val_loss: 2.0325 - 3s/epoch - 19ms/step\n",
            "Epoch 18/30\n",
            "\n",
            "Epoch 18: val_loss improved from 2.03251 to 1.99973, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.1859 - val_loss: 1.9997 - 3s/epoch - 19ms/step\n",
            "Epoch 19/30\n",
            "\n",
            "Epoch 19: val_loss improved from 1.99973 to 1.98441, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.1001 - val_loss: 1.9844 - 3s/epoch - 19ms/step\n",
            "Epoch 20/30\n",
            "\n",
            "Epoch 20: val_loss improved from 1.98441 to 1.95483, saving model to model.h5\n",
            "141/141 - 3s - loss: 1.0207 - val_loss: 1.9548 - 3s/epoch - 19ms/step\n",
            "Epoch 21/30\n",
            "\n",
            "Epoch 21: val_loss improved from 1.95483 to 1.91884, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.9423 - val_loss: 1.9188 - 3s/epoch - 18ms/step\n",
            "Epoch 22/30\n",
            "\n",
            "Epoch 22: val_loss improved from 1.91884 to 1.90507, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.8713 - val_loss: 1.9051 - 3s/epoch - 19ms/step\n",
            "Epoch 23/30\n",
            "\n",
            "Epoch 23: val_loss improved from 1.90507 to 1.89451, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.8008 - val_loss: 1.8945 - 3s/epoch - 19ms/step\n",
            "Epoch 24/30\n",
            "\n",
            "Epoch 24: val_loss improved from 1.89451 to 1.87838, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.7354 - val_loss: 1.8784 - 3s/epoch - 19ms/step\n",
            "Epoch 25/30\n",
            "\n",
            "Epoch 25: val_loss improved from 1.87838 to 1.86650, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.6796 - val_loss: 1.8665 - 3s/epoch - 19ms/step\n",
            "Epoch 26/30\n",
            "\n",
            "Epoch 26: val_loss improved from 1.86650 to 1.85334, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.6209 - val_loss: 1.8533 - 3s/epoch - 19ms/step\n",
            "Epoch 27/30\n",
            "\n",
            "Epoch 27: val_loss improved from 1.85334 to 1.84144, saving model to model.h5\n",
            "141/141 - 3s - loss: 0.5682 - val_loss: 1.8414 - 3s/epoch - 19ms/step\n",
            "Epoch 28/30\n",
            "\n",
            "Epoch 28: val_loss did not improve from 1.84144\n",
            "141/141 - 3s - loss: 0.5235 - val_loss: 1.8459 - 3s/epoch - 18ms/step\n",
            "Epoch 29/30\n",
            "\n",
            "Epoch 29: val_loss did not improve from 1.84144\n",
            "141/141 - 2s - loss: 0.4777 - val_loss: 1.8481 - 2s/epoch - 17ms/step\n",
            "Epoch 30/30\n",
            "\n",
            "Epoch 30: val_loss did not improve from 1.84144\n",
            "141/141 - 2s - loss: 0.4379 - val_loss: 1.8590 - 2s/epoch - 18ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f59ba762b10>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n",
        "\n",
        "def max_length(lines):\n",
        "    return max(len(line.split()) for line in lines)\n",
        "\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "    X = tokenizer.texts_to_sequences(lines)\n",
        "    X = pad_sequences(X, maxlen=length, padding='post')\n",
        "    return X\n",
        "\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "    prediction = model.predict(source, verbose=0)[0]\n",
        "    integers = [argmax(vector) for vector in prediction]\n",
        "    target = list()\n",
        "    for i in integers:\n",
        "        word = word_for_id(i, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "def evaluate_model(model, sources, raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    for i, source in enumerate(sources):\n",
        "        source = source.reshape((1, source.shape[0]))\n",
        "        translation = predict_sequence(model, eng_tokenizer, source)\n",
        "        raw_target, raw_src = raw_dataset[i]\n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "        actual.append(raw_target.split())\n",
        "        predicted.append(translation.split())\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "dataset = load_clean_sentences('english-german-both.pkl')\n",
        "train = load_clean_sentences('english-german-train.pkl')\n",
        "test = load_clean_sentences('english-german-test.pkl')\n",
        "\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])"
      ],
      "metadata": {
        "id": "PQHKmDzK39Z-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model, trainX, train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZiHZJdB34QF",
        "outputId": "30c74b59-c3e3-4fb8-a4d9-c4508789fe4f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src=[bedien dich], target=[help yourself], predicted=[thanks yourself]\n",
            "src=[komm und besuch mich], target=[come see me], predicted=[come see me]\n",
            "src=[druckt mir die daumen], target=[wish me luck], predicted=[wish me luck]\n",
            "src=[ich unterstutze ihn], target=[i support him], predicted=[i support him]\n",
            "src=[ich bin pingelig], target=[im finicky], predicted=[im picky]\n",
            "src=[das ist aus gold], target=[this is gold], predicted=[this is gold]\n",
            "src=[gib ihn tom], target=[hand it to tom], predicted=[hand it to tom]\n",
            "src=[es ist ihre pflicht], target=[its your duty], predicted=[its your duty]\n",
            "src=[siehe unten], target=[see below], predicted=[see below]\n",
            "src=[wir haben einen hund], target=[we have a dog], predicted=[we have a car]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.084573\n",
            "BLEU-2: 0.275912\n",
            "BLEU-3: 0.442778\n",
            "BLEU-4: 0.498356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val = 5\n",
        "source = trainX[val].reshape((1, trainX[val].shape[0]))\n",
        "translation = predict_sequence(model, eng_tokenizer, source)\n",
        "train[val][1],train[val][0],translation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBkhQoZUmC_R",
        "outputId": "09dcc473-71f3-4983-a94a-ed1c181a1d55"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('das ist aus gold', 'this is gold', 'this is gold')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}